<?xml version='1.0' encoding='utf-8'?>
<thread>
  <submission id="2050475588.0" index="1" link="https://www.reddit.com//r/changemyview/comments/xwst38/cmv_the_only_difference_between_ai_digital_life/">
    <title>CMV: The only difference between AI (digital) life and human (physical) life is the material which sustains it</title>
    <original_poster>CarpeBedlam</original_poster>
    <original_post>To be clear, I’m talking about a theoretical AI which is designed specifically to be indistinguishable from a human, and accomplishes that goal. Also, I don’t necessarily mean an android that is physically equivalent (e.g. Ash from “Alien”), but just a digital consciousness living in a computer which is capable of replicating human behavior. At that point, I’m struggling to find a difference between the two that isn’t solely about the material required to sustain the life (blood, tissue, etc. for human life, silicon, copper, etc. for AI life).</original_post>
    </submission>
  <comment id="40829427397.0" user="halavais" delta="True">
    <text>1. You are assuming a pretty stark material/organizational divide here.

The truth is that part of what makes a human brain act so human is the constraints and affordances of the materials that it is made of. And the process by which a brain is made does not make the same kinds of stark divisions between code and hardware (which are, after all, an abstraction mainly for us, as humans, to think about complex machines). That lack of clear division makes the idea of hardware/software difficult in humans, and--as a practical matter--in (other) machines.

2. Is this just the Turing test?

That is to say, does simulation to a high degree of precision result in no practical differences regardless of the system simulating it? If so, it seems to me to be a question of a different sort, though it continues to be a discussion. (See, for example, Searle's Chinese Room.)

3. A difference that makes a difference?

Two humans can think and behave in quite different ways. Presumably you are not asking for the intelligence to behave or act identically to a single given human.

So that leaves the question not of difference, but how much, and whether it is a difference that makes a difference... to something. What is the meaningful outcome of this lack of difference? Is this a question because we would want it reflected in the language we use? In the rights granted to the entity? In the restrictions and responsibilities put on it?

In other words: there is bound to be a difference between a given artificial human mind and a human mind. Even with the assumption that we could make an instantaneous physical copy of an individual human's mind, there is every reason to believe that the two minds would diverge rapidly. So the question is how much difference is enough to matter, and that means specifying an outcome that such difference might affect.</text>
    </comment>
  </thread>
