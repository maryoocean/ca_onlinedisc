<?xml version='1.0' encoding='utf-8'?>
<thread>
  <submission id="2050475588.0" index="1" link="https://www.reddit.com//r/changemyview/comments/xwst38/cmv_the_only_difference_between_ai_digital_life/">
    <title>CMV: The only difference between AI (digital) life and human (physical) life is the material which sustains it</title>
    <original_poster>CarpeBedlam</original_poster>
    <original_post>To be clear, I’m talking about a theoretical AI which is designed specifically to be indistinguishable from a human, and accomplishes that goal. Also, I don’t necessarily mean an android that is physically equivalent (e.g. Ash from “Alien”), but just a digital consciousness living in a computer which is capable of replicating human behavior. At that point, I’m struggling to find a difference between the two that isn’t solely about the material required to sustain the life (blood, tissue, etc. for human life, silicon, copper, etc. for AI life).</original_post>
    </submission>
  <comment id="40828598650.0" user="BeardedSmitty" delta="False">
    <text>Feelings and emotions? Much of what we do is derived from emotion or our feelings. An ai system cannot change how they feel because they cannot feel anything. They just calculate their next path based on what it is told to do.</text>
    </comment>
  <comment id="40828639300.0" user="BicameralProf" delta="False">
    <text>Emotions are ultimately just neurons firing in the brain. I don't see why that couldn't be replicated in a digital mind if we can replicate things like language ability which is also just neurons firing in the brain.</text>
    </comment>
  <comment id="40828926062.0" user="TripRichert" delta="False">
    <text>we've got good training data sets on language.

detailed data training sets on emotion seem like they would be much harder to produce.</text>
    </comment>
  <comment id="40828936481.0" user="BicameralProf" delta="False">
    <text>There might not be anyone who has done it yet but it seems like it'd be relatively easy to use music, movies, poetry, etc. to create a data training set for emotion.</text>
    </comment>
  <comment id="40829016703.0" user="TripRichert" delta="False">
    <text>&amp;gt; it seems like it'd be relatively easy to use music, movies, poetry, etc. to create a data training set for emotion

I think you and I have very different definitions of what "data training set" means.</text>
    </comment>
  <comment id="40830304376.0" user="BicameralProf" delta="False">
    <text>Care to elaborate? What is your definition of a data training set that excludes the things I listed?</text>
    </comment>
  <comment id="40828641139.0" user="oceanblu456" delta="False">
    <text>I think he’s saying if a AI is able to effectively mimic the understanding of emotions then what is the difference? Your perceived concept of consciousness. But if a AI is able to process consequence to the same the degree as humans then what is the difference?</text>
    </comment>
  <comment id="40829611113.0" user="CarpeBedlam" delta="False">
    <text>Yes, thank you for expressing it in different terms. I feel like consciousness must be something more than just reacting to new data based on our existing datasets the way an AI would, but I can’t find any evidence to support that feeling. This is leading me to wonder if consciousness has physical properties and limitations, or could it be nonexistent, at least in any terms we currently think of it.</text>
    </comment>
  <comment id="40828648732.0" user="CarpeBedlam" delta="False">
    <text>Couldn’t it be said that feelings and emotions are reactions/responses to incoming data? That there is a process involved, and therefore is not unlike a process an AI would follow to react/respond to similar incoming data?</text>
    </comment>
  <comment id="40828668307.0" user="BeardedSmitty" delta="False">
    <text>You've got a point there. But if one of my young children dies, I know for a fact I would not be able to function 100% for a long long time. An ai system would "feel" sad in that moment but the action of a "child" dying would not be involved with it's next "thought" of what it is going to do next. It would just do whatever it's next task is uninterrupted by guilt, grievance, or sorrow.</text>
    </comment>
  <comment id="40828731326.0" user="axis_next" delta="False">
    <text>I mean if it is indeed successfully designed to have behaviour indistinguishable from a human, then probably it would have to include effects like those. But I cannot fathom why anyone would specifically seek to design that, and I don't think it would necessarily land on it "naturally". Although emotions like that do serve a function, and the work of processing them might well impair other kinds of processing happening simultaneously.</text>
    </comment>
  <comment id="40829305854.0" user="CarpeBedlam" delta="False">
    <text>The human reaction to death can vary greatly between individuals, but also between cultures, philosophies, circumstances, etc. I would say that the reaction, whether indifference or prolonged grief, is based on the individual’s “programming” thus it could, theoretically, be learned by an AI.</text>
    </comment>
  <comment id="40832000775.0" user="ElysiX" delta="False">
    <text>&amp;gt;An ai system would "feel" sad in that moment but the action of a "child" dying would not be involved with it's next "thought" of what it is going to do next.

What makes you say that? Why wouldn't we be able to program it such that in that case, it's next task IS guilt, grievance or sorrow? Or phrased differently, tune it's weights such that it is prone to emotional breakdowns that linger for a while?

I'd say we already have that, AI systems acting totally out of line and incorrectly because some neuron of theirs has received an unprecedently high value, and they essentially go into an emotional breakdown and produce complete nonsense as output until it leaves their attention span.</text>
    </comment>
  <comment id="40828869157.0" user="Dr_Czarbarian" delta="False">
    <text>Not entirely. Hormones are involved, too. If an AI isn't getting a dose of hormones at the same intensity, can it really be the same feeling?</text>
    </comment>
  </thread>
