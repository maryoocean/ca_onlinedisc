<?xml version='1.0' encoding='utf-8'?>
<thread>
  <submission id="1964912436.0" index="45" link="https://www.reddit.com//r/changemyview/comments/whuw50/cmv_k12_educators_push_a_false_narrative_that_all/">
    <title>cmv: K-12 Educators push a false narrative that all college degrees are good investments</title>
    <original_poster>Suitable-Ad-8598</original_poster>
    <original_post>Students in high school are bombarded with pressure to go to college by educators, with high emphasis on the ranking of the schools they are accepted into and no emphasis on the majors that they choose.

Nobody is warning students that picking liberal arts majors (unless they go to an ivy league school or something like it), will probably make them financially worse off then not even getting a degree in the first place. I know a girl that did gender studies at Columbia and is working as a barista several years out of college and is in about $400,000 in debt. I went to a way worse school than Columbia and nearly all of my friends with liberal arts degrees are stuck working as cashiers and in food service. I know a ton of people who went to community colleges (which was highly frowned upon in my high school culture) who are making six figures a couple years out of school (STEM and sales).

Why are we not being honest with these kids about the consequences of incurring student debt in unusable majors? I am not saying people shouldn't have the right to make their own major choices, but this is like handing children a pack of cigarettes and not telling them it causes cancer.

Edit: I probably exaggerated when I said worse than no degree if we are talking about long-term in their lives—I kind of meant in place of certifications or bootcamps.  What I should have said is that the lack of jobs in liberal arts fields seems to not be communicated to kids, which has significant negative financial consequences. 

&amp;amp;#x200B;

&amp;amp;#x200B;

&amp;amp;#x200B;</original_post>
    </submission>
  <comment id="40350700377.0" user="BrotherBodhi" delta="False">
    <text>My experience is anecdotal, and I understand it probably is not the norm. But I grew up in a religious school, and never once did someone tell me that I should go to college. In the evangelical world college is seen as some dangerous place where kids go and lose their faith. My parents never talked to me about college, because no one in my family had ever gone to college. By the time I was in my mid 20s with a child and struggling to make it month to month working multiple entry level jobs, I really wished someone had sat me down and talked to me about college when I was a teen. That’s probably my biggest regret in life honestly. I have managed to claw my way up a little bit, but it feels like I will be in an eternal financial struggle for the sole reason that I didn’t get an education. If I could go back and change one thing about my teenage years, I’d have myself be bombarded by educators and pressured to go to college.</text>
    </comment>
  </thread>
